\documentclass[12pt]{article}

\usepackage{graphics}
\usepackage{graphicx}

\setlength{\tabcolsep}{4pt}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[T1]{fontenc}
\usepackage[usenames,dvipsnames]{color}
%\usepackage[utf8x]{inputenc}
\usepackage{a4wide}
\usepackage{afterpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{caption}\DeclareCaptionType{copyrightbox}
\usepackage{enumitem}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{scalerel}
%\usepackage{subfig}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{url}
\usepackage{verbatim} 
\usepackage{xcolor}

\usepackage{mathtools}

\usepackage{subfigure}

\include{definitions}

\begin{document}

\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\lipsumcolor}[1]{{\color{blue}\lipsum[#1]}}

\title{P-score: a complementary metric to H-index} 

\author{Jo\~ao Veneroso}
%\affiliation{
%  \institution{CS Dept, UFMG}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{ueda@dcc.ufmg.br}

\author{Marlon Dias}
%\affiliation{
%  \institution{CS Dept, UFMG}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{msdias@dcc.ufmg.br}

\author{Berthier Ribeiro-Neto}
%\affiliation{
%  \institution{CS Dept, UFMG \& Google Inc}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{berthier@dcc.ufmg.br}

\author{Nivio Ziviani}
%\affiliation{
%  \institution{CS Dept, UFMG \& Kunumi}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{nivio@dcc.ufmg.br}

\author{Edmundo de Souza e Silva}
%\affiliation{
%  \institution{CS Dept, UFRJ}
%  \city{Rio de Janeiro} 
%  \country{Brazil} 
%}
%\email{edmundo@land.ufrj.br}

% If the default list of authors is too long for headers}
%\renewcommand{\shortauthors}{A. Ueda, M. Dias, B. Ribeiro-Neto, N. Ziviani, and E. de Souza e Silva}

% To anonymize:
% \begin{anonsuppress} \end{anonsuppress}

\begin{abstract}
The notion of reputation in academia is critical for taking decisions on research grants, faculty position tenure, 
and research excellence awards. And the notion of reputation is always associated with the publication track 
record of the researcher or research group. Thus, it is important to assess publication track records 
quantitatively. To quantify a publication record, bibliographic metrics are usually adopted. Among these, citation 
based metrics, such as H-indices and citation counts, are quite popular. In this paper we study the correlation between 
P-score, a publication record metric we introduced previously, and H-indices. We show that they are correlated with 
a Kendall-Tau coefficient that exceeds 0.5. Additionally, we noticed that they have important differences. 
We were able to identify publication venues with high H-indices and low P-scores, as well as venues with low 
H-indices and high P-scores. We provide interpretations for these findings and discuss how they can be 
used by research funding councils and committees to better support their funding decisions. 
\end{abstract}

% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10002951.10003317.10003338</concept_id>
%<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Information systems~Retrieval models and ranking}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\keywords{Academic search, random walks, reputation flows}

\maketitle

\section{Introduction}
\label{sec:the-problem}

Academic research evaluation is a topic of interest to universities, research groups,
research funding institutions and the public at large. The evaluation of research
is usually carried out by comparing journals, conferences or papers through the usage of 
academic impact metrics. A popular and moderately effective approach is to compute 
metrics based on the number of citations received by a given piece of research and 
assume that the number of citations is a good proxy for research quality. However,
this approach has multiple shortcomings. 

First, in the evaluation and bibliometrics research communities, citations are understood 
as a measure of attention rather than a proxy for impact or quality. Citations measure 
the attention to a paper of peers in related fields \cite{le16}, not the quality of the 
work produced.

Second, citations can take a long time to happen. To illustrate this, in a recent study that 
looked at first time to citation in a universe of more than a million papers, half the 
papers only received their first citation 20 months or more after they were published \cite{n16}. 

Third, citations are not simple to compute and are not always broadly available, particularly 
at the level of individual researchers. Collecting citation counts requires access to the 
contents of a large number of publications, of which many are not easily available, and 
most authors do not have up to date citation information in their public internet profiles.

Despite these limitations, citation based metrics continue to be a popular approach to
academic research evaluation. Nonetheless, it is desirable to employ alternative and
complementary metrics that tackle the exposed problems without loss of effectiveness.

Academic reputation is an individual or group property strongly associated with 
academic impact. Reputable venues tend to concentrate the most relevant research papers 
because that is how they acquire and keep their good reputations. At the same time, 
reputable authors seek to publish on reputable venues because it gives their 
research more visibility and accreditation. Therefore, a researcher conveys reputation
to a venue proportionally to its own reputation and the reputation of a researcher
is proportional to the reputation of the venues where she or he publishes.
This relationship between authors and venues constitutes a reputation network in which 
authors influence venue reputations and vice versa. 

Pscore is a graph modeling metric that attributes quantitative reputations to venues 
based on the publication patterns of a reference group of researchers without having to
rely on citation information. It deals with the three problems discussed previously. 

First, instead of measuring the amount of researcher attention, Pscore measures reputations,
which is presumably a better proxy for academic impact as was shown by Ribas.

Second, while citations can take years to happen, reputations tend to be steady over 
the time, so the quality of new research can be promptly estimated with pscore by 
simply complementing the reputation flows graph with the newest publication data.

Third, pscore does not require citation data to perform effectively, thus we can avoid
the laborious effort of compiling individual researcher citation information.

The Pscore of computer science venues has shown significant correlation with H-index. Yet, 
there are important cases where the two metrics diverge considerably. Namely, in venues 
that belong to an intersection between different domains and in venues of less popular 
subareas of computer science. These differences appeal to the importance of pscore as a 
complementary metric.

\section{P-score: a network-based metric} 

Contrary to citation counting metrics such as the Impact 
Factor~\cite{saha2003,garfield1996,thomson2011,balaban2012positive} and H-index~\cite{benevenuto2016h,bar-ilan2007,egghe2008,bornmann2005does,bornmann2011h}, P-scores are a graph modeling metric that takes into account 
the relations among researchers, papers they published and their publication venues. They are 
based on a framework of reputation flows we introduced previously~\cite{rrsz2015}. Let us review them 
briefly. 

A \emph{reputation graph} in academia is a graph with three node types: (a) {\em reputation sources} representing groups of selected researchers, 
(b) {\em reputation targets} representing venues of interest, and
(c) {\em reputation collaterals} representing entities we want to compare such as research groups and academic 
departments. 
Figure~\ref{fig:overview} provides a generic illustration of our reputation graph and introduces the following 
notation: $S$ is the set of reputation sources, $T$ is the set of reputation targets, and $C$ is the set of reputation collaterals.
We first propagate the reputation of source nodes to target nodes, which allows assigning 
weights to the target nodes. Following, we propagate these weights to the collaterals. 
In here we adopt research groups as source nodes and publication venues as target nodes. 
Further we focus on the weights assigned to the publication venues and do not consider 
potential collaterals of interest (such as individual researchers). 

\begin{figure}[h]
   \centerline{\includegraphics[scale=0.75]{figures/overview-line}}
   %\centerline{\includegraphics[scale=0.6]{figures/models/overview}}
   \caption{Structure of the reputation graph.}
   \label{fig:overview}
\end{figure}

Our reputation graph for academia can be naturally associated with paper authors, modelled as reputation 
sources, and papers, modelled as reputation targets. 
The interaction between reputation sources and reputation targets is inspired by the 
notion of {\em eigenvalue centrality} in complex 
networks~\cite{DBLP:journals/cn/BrinP98,Meyer-pagerank06,Newman-book}. 
In the reputation graph, if we consider only sources and targets, it is easy to identify reputation flows from sources to sources, from sources to targets, from targets to sources, and from targets to targets. These reputation flows can be modeled as a stochastic process. 
In particular, let $P$ be a \emph{right stochastic} %\footnote{A right stochastic matrix is ...} 
matrix of size $(|S|+|T|) \times (|S|+|T|)$ with the following structure: 

\newcommand{\bkt}[1]{ {^{\langle #1 \rangle}} }

\begin{align}\label{eq:newp}
%\[
P =
\left[
\begin{array}{r | r}
(d\bkt{S}) . P\bkt{SS}  & (1-d\bkt{S}) . P\bkt{ST} \\
\hline
(1-d\bkt{T}) . P\bkt{TS}  & (d\bkt{T}) . P\bkt{TT} \\
\end{array}
\right]
%\]
\end{align}
\noindent where each quadrant represents a distinct type of reputation flow, as follows: 

\begin{description}
\item $P\bkt{SS}$: right stochastic matrix of size $|S|\times |S|$ representing the transition probabilities between reputation sources;
\item $P\bkt{ST}$: matrix of size $|S|\times |T|$ representing the transition probabilities from reputation sources to targets;
\item $P\bkt{TS}$: matrix of size $|T|\times |S|$ representing the transition probabilities from reputation targets to sources;
\item $P\bkt{TT}$: right stochastic matrix of size $|T|\times |T|$ representing the transition probabilities between reputation targets.
\end{description}

The parameters $d\bkt{S}$, the fraction of reputation one wants to transfer among the source nodes themselves, 
and $d\bkt{T}$, the fraction of reputation one wants to transfer among the target nodes themselves,
control the relative importance of the reputation sources and targets. 
Assuming that the transition matrix $P$ is ergodic, we can compute the steady state probability of each node and use it as a reputation score. 
More formally, we can write: 
\begin{align}
\label{eq:ggP}
\gamma = \gamma P
\end{align}
\noindent where $\gamma$ is a row matrix with $|S|+|T|$ elements, 
where each row represents the transition probabilities of a node in the set $S\cup T$. 

We should note that, while our network model allows modeling citations in the fourth quadrant, it is possible to compute 
steady state probabilities for the network without consideration to citations. This is accomplished by setting the parameter 
$d\bkt{T} = 0$. 
Thus, it should be clear, in all of our experiments here P-scores 
are computed without taking citations into account. 

%%%%%%%%%%%%% 
\subsection{Reputation Sources}
\label{sec:rep-sources}

The choice of the reputation sources is an important part of the method since its composition has a direct impact in the final rankings. There is no definitive way to make it. This choice depends on what we want to measure. In here we 
use the top CS departments in the US as reputation sources. It is a simple procedure that allows assigning P-scores 
to publication venues, P-scores that 
reflect the publication patterns of top CS departments in the US. 
We then compare how these scores compare with H-indices assigned to the same publication venues. 

One way to determine the top CS departments is to 
adopt the randomization procedure we first described in~\cite{rrsz2015}. A run of that 
procedure works as follows: 
\begin{enumerate}
\item randomly select 10 entities from the set of reputation targets and use them as the set $S$ of reputation sources
\item compute steady state probabilities for all nodes
\item using the steady state probabilities of reputation targets as a score, select the 10 entities with highest scores and use them as a new set $S_{new}$ of reputation sources
\item if $S_{new} \not \equiv S$ then $S \leftarrow S_{new}$ and go back to step 1
\item $S_{auto} \leftarrow S_{new}$ 
\item take $S_{auto}$ as the set of automatically selected reputation sources
\item exit
\end{enumerate}
By applying this randomization procedure 100 times to a set of 125 US graduate programs in Computer Science (CS), 
exactly the same 125 graduate programs considered by NRC in its 2010 evaluation of CS graduate programs 
in the USA~\cite{nrc}, we ended up with a subset of 12 CS programs to be considered as reputation sources. 
These are the CS programs that appeared as least once in the set $S_{auto}$ of automatically selected reputation 
sources and they are as follows: 
%%%
\begin{enumerate}
\item Carnegie Mellon University
\item Georgia Institute of Technology
\item Massachusetts Institute of Technology
\item Stanford University
\item University of California-Berkeley
\item University of California-Los Angeles
\item University of California-San Diego
\item University of Illinois at Urbana-Champaign
\item University of Maryland College Park
\item University of Southern California
\item University of Michigan-Ann Arbor
\item Cornell University
\end{enumerate}
%%%

All the 12 departments above are among the top 5th percentile in the ranking produced by NRC. 
This suggests that our recursive procedure is able to take advantage of patterns in the publication 
streams of the various CS departments to determine the most reputable ones in fully automatic fashion.
We further observe that this was done while setting the parameter $d\bkt{T} = 0$. That is, we did 
not use information on citation counts in the model. 

\section{Correlation with H-indices} 

{\bf TO WRITE: explain that distributions are not Normal, show graphs, propose Kendall-Tau, show macro values}

\section{Assessing conferences in CS}
\label{sec:notifications}

{\bf Discuss the cone strategy, justify the 10 degrees angle, discuss the problem of positioning the cone 
vertex at the origin, introduce the (-100,-100) pivot, discuss items above and below the cone}

\section{Related Work}\label{sec:related-work}

{\bf REWRITE: adjust for theme of this paper}

%%%%%
%%% Ranking academic entities in the broad areas %%%
%%%%%

Garfield's Impact Factor \cite{garfield1955} is a journal-level metric that accounts for the mean number 
of citations received by articles published in a given journal over a 2-year period. Despite being one of 
the earliest approaches at measuring academic impact, it has showed remarkable survivability. Pinski et. al. 
\cite{Pinski1976} describe several limitations with the metric. It also suffered criticism for 
being misleading \cite{nature2016, Saha2003} and for lacking reliable validation from an independent 
audit \cite{rossner2007}. Riikonen and Vihinen (2008) \cite{Riikonen2008} showed that actual citation 
and publication counts were better predictors of a scientist's contribution than impact factors.

The acceptance ratio \cite{Chen2010} is the proportion of accepted papers relative to the number of submitted
papers.

Citation half-life


The H-index is an author-level metric proposed by Hirsch \cite{hirsch2005} in 2005. Since then, it has been 
widely adopted as a measure of invidual scientific research output. An h-index of 
$ h $ means that a researcher has published $ h $ papers that were cited at least $ h $ times.
The H-index takes into account both the number of publications and the number of citations per publication, 
achieving higher values when a researcher obtains a consistently high number of citations over multiple publications. 
The H-index has been criticized for not taking into account field specific citation
statistics \cite{wendl2007}. A researcher may have a misleading high or low h-index because her or his
field has an unusually high or low average number of citations due to differences in popularity.
In 2006, Egghe propsed the g-index \cite{Egghe2006}, a less strict variation of the h-index where the score
$ g $ is the largest number such that the top $ g $ articles received at least $ g^2 $ citations.

Citation counts and derived metrics are straightforward to calculate, but they represent only a coarse
estimate of academic impact because they are essentialy popularity measures. Citations from 
prestigious journals are more relevant than citations from peripheral journals, as noted by
Pinski et. al \cite{Pinski1976}. To address this issue, some studies distinguish popularity from 
reputation or prestige with the usage of citation weighting strategies \cite{Ding2011, Yan2011} or 
PageRank based metrics \cite{Bollen2006, Sun2007}.

Furthermore, Piwowar (2013) \cite{Piwowar2013} noted that citation based metrics are slow, since
the first citation to a scientific article can take years to happen. 


Leydesdorff (2009) \cite{Leydesdorff2009} compared traditional indicators such as impact factor
imediacy index and cited half-life to new indicators such as H-index, ***SJR*** and PageRank.

Martins et. al. (2009) proposed a method of assessing the quality of scientific conferences 
through a machine learning classifier that makes use of multiple features in addition to citations.

The idea of reputation, without the direct use of citation data, was discussed by Nelakuditi et 
al. \cite{Nelakuditi2011}. They proposed a metric called \textit{peers' reputation} for 
research conferences and journals, which ties the selectivity of the publication venue based upon 
the reputation of its authors' institutions. The proposed metric was shown to be a better indicator 
of the selectivity of a research venue than acceptance ratio.





Regarding the assessment of individual researchers' influence and expertise, many approaches have 
been introduced~\cite{balog2012ftir, cormode2014people, deng2012modeling, gollapalli2011ranking, wu2009detecting}. 
Particularly, 


Gonçalves et al.~\cite{goncalves2014} quantified the impact of various features on a scholar popularity throughout her career. 
They concluded that, even though most of the considered features are strongly correlated with popularity, only two features are needed to explain almost all the variation in popularity across different researchers: the number of publications and the average quality of the scholar’s publication venues. 
In addition, the prediction of scientific success of a researcher is also valuable for several goals
~\cite{masoumeh16}. As a result, previous works attempted to predict if a researcher will become a principal investigator~\cite{vandijk14}, her future H-index~\cite{dong15, penner13} and the potential number of citations to her publications~\cite{mazloumian12, castillo2007}.




The reputation model we use in this work was proposed 
in 
~\cite{ribas2015random}. This model, called \textit{reputation flows}, exploits the transference of reputation among entities in order to identify the most reputable ones. Particularly, the reputation flows consist in a random walk model where the reputation of a target set of entities is inferred using suitable sources of reputation. To evaluate this model, they instantiated the reputation flows in an academic setting, proposing a novel metric for academic reputation, the \textit{P-score}~\cite{ribas2015bigscholar}. 

By and large, the aforementioned works or variations of them are commonly used in assessments of academic output and also by modern search engines for scientific digital libraries, e.g. Google Scholar\footnote{\url{https://scholar.google.com.br}}, Microsoft Academic Search\footnote{\url{http://academic.microsoft.com}}, AMiner\footnote{\url{http://aminer.org}}, %maintained by Tang et al.~\cite{Tang:08KDD},
and CiteSeerX\footnote{\url{http://citeseerx.ist.psu.edu}}.  
However, %all of those performance 
none of the referred metrics take into account the different publication patterns in the subareas. Studies suggesting those differences and the negative impact of uniform evaluation metrics have been discussed in the field of Economics~\cite{kapeller10, lee10} and in Computer Science~\cite{hoonlor13, laendersigs15, lima2013jcdl}.

Wainer et al.~\cite{wainer13} presented the first attempt to quantify the differences in publication and citation practices between the subareas of Computer Science. 
Their key findings were: i) there are significant differences in productivity across some CS subareas, both in journals (e.g. Bioinformatics has a significantly higher productivity than Artificial Intelligence) and in conferences (e.g. Image Processing and Computer Vision has a significantly higher productivity than Operational Research and Optimization); ii) the mean number of citations per paper varies depending on subarea (e.g. Management Information Systems has significantly higher citation rates per paper than Computer Architecture); and iii) there are significant differences in emphasis on publishing in journals or in conferences (e.g. Bioinformatics are clearly journal oriented while Artificial Intelligence are conference oriented). However, they do not focus on modeling a new productivity metric for academic domain taking into account those differences between the subareas.

\section{Conclusions}\label{sec:conclusions}

ZZZ Rewrite entirely.

\begin{comment}
\begin{acks}

ZZZ Thank scholarships by CAPES and CNPq.

\end{acks}
\end{comment}
%% To fill space (Ueda)
\section*{ACKNOWLEDGEMENTS}
Omitted for blind review.

%\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{abbrv}
\bibliography{pscore}

\end{document}

