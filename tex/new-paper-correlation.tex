\documentclass[12pt]{article}

\usepackage{graphics}
\usepackage{graphicx}

\setlength{\tabcolsep}{4pt}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[T1]{fontenc}
\usepackage[usenames,dvipsnames]{color}
%\usepackage[utf8x]{inputenc}
\usepackage{a4wide}
\usepackage{afterpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{caption}\DeclareCaptionType{copyrightbox}
\usepackage{enumitem}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{scalerel}
%\usepackage{subfig}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{url}
\usepackage{verbatim} 
\usepackage{xcolor}

\usepackage{mathtools}

\usepackage{subfigure}

\include{definitions}

\begin{document}

\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\lipsumcolor}[1]{{\color{blue}\lipsum[#1]}}

\title{P-score: a complementary metric to H-index} 

\author{Jo\~ao Veneroso}
%\affiliation{
%  \institution{CS Dept, UFMG}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{ueda@dcc.ufmg.br}

\author{Marlon Dias}
%\affiliation{
%  \institution{CS Dept, UFMG}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{msdias@dcc.ufmg.br}

\author{Berthier Ribeiro-Neto}
%\affiliation{
%  \institution{CS Dept, UFMG \& Google Inc}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{berthier@dcc.ufmg.br}

\author{Nivio Ziviani}
%\affiliation{
%  \institution{CS Dept, UFMG \& Kunumi}
%  \city{Belo Horizonte} 
%  \country{Brazil} 
%}
%\email{nivio@dcc.ufmg.br}

\author{Edmundo de Souza e Silva}
%\affiliation{
%  \institution{CS Dept, UFRJ}
%  \city{Rio de Janeiro} 
%  \country{Brazil} 
%}
%\email{edmundo@land.ufrj.br}

% If the default list of authors is too long for headers}
%\renewcommand{\shortauthors}{A. Ueda, M. Dias, B. Ribeiro-Neto, N. Ziviani, and E. de Souza e Silva}

% To anonymize:
% \begin{anonsuppress} \end{anonsuppress}

\begin{abstract}
The notion of reputation in academia is critical for taking decisions on research grants, faculty position tenure, 
and research excellence awards. And the notion of reputation is always associated with the publication track 
record of the researcher or research group. Thus, it is important to assess publication track records 
quantitatively. To quantify a publication record, bibliographic metrics are usually adopted. Among these, citation 
based metrics, such as H-indices and citation counts, are quite popular. In this paper we study the correlation between 
P-score, a publication record metric we introduced previously, and H-indices. We show that they are correlated with 
a Kendall-Tau coefficient that exceeds 0.5. Additionally, we noticed that they have important differences. 
We were able to identify publication venues with high H-indices and low P-scores, as well as venues with low 
H-indices and high P-scores. We provide interpretations for these findings and discuss how they can be 
used by research funding councils and committees to better support their funding decisions. 
\end{abstract}

% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10002951.10003317.10003338</concept_id>
%<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Information systems~Retrieval models and ranking}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\keywords{Academic search, random walks, reputation flows}

\maketitle

\section{Introduction}
\label{sec:the-problem}

Academic research evaluation is a topic of interest to universities, research groups,
research funding institutions and the public at large. The evaluation of research
is usually carried out by comparing journals, conferences or papers through the usage of 
academic impact metrics. A popular and moderately effective approach is to compute 
metrics based on the number of citations received by a given piece of research and 
assume that the number of citations is a good proxy for research quality. However,
this approach has multiple shortcomings. 
First, in the evaluation and bibliometrics research communities, citations are understood 
as a measure of attention rather than a proxy for impact or quality. Citations measure 
the attention to a paper of peers in related fields \cite{le16}, not the quality of the 
work produced.
Second, citations can take a long time to happen. To illustrate this, in a recent study that 
looked at first time to citation in a universe of more than a million papers, half the 
papers only received their first citation 20 months or more after they were published \cite{n16}. 
Third, citations are not simple to compute and are not always broadly available, particularly 
at the level of individual researchers. Collecting citation counts require access to the 
contents of a large number of publications, of which many are not easily available, and 
most authors do not have up to date citation information in their public profiles.
Despite these limitations, citation based metrics continue to be a popular approach to
academic research evaluation. Nonetheless, it is desirable to employ alternative and
complementary metrics that tackle the exposed problems without loss of effectiveness.

Academic reputation is an individual or group property strongly associated with 
academic impact. Reputable venues tend to concentrate the best research papers 
because that is how they acquire and keep their good reputations. At the same time, 
reputable authors seek to publish in reputable venues because it gives their 
research more visibility and accreditation. 



This relationship between authors and
venues constitutes a reputation network in which authors influence venue reputations
and vice versa. Pscore is a graph modeling metric that attributes quantitative reputations 
to venues based on the publication patterns of a reference group of researchers.





concentrate better research 

papers because that is why they 
first acquire their good reputations and 


The notion of academic productivity is intrinsically associated
with the notion of reputation.


Pscore is network-based metric that does not rely on citation counts 


For instance, while citations are not always available at the level of individual researchers, 
they are widely available at the level of journals and research conferences, that is, at the 
level of publication venues. This is of interest if one takes Despite that, weighting publications 
by venue-level metrics is a far simpler and more direct 
approach than computing citations for individual researchers. Because of that, venue-level 
metrics are widely used and are of relevance~\cite{laender2008,le16}. 


These shortcomings point to the need of a different academic impact metric. 


{\bf REWRITE (explain the need for additional metrics, such as P-scores)\\}

Evaluating impact of research is of interest to universities, research groups, 
research funding institutions, governments, and the public at large. 
And while there are many facets to consider while evaluating the impact of research, 
a simple approach is to compute metrics based on the number of citations a given piece of research receives. 
The popularity of citation based metrics derives from their 
simple definition and consequent 
intuitive appeal. Despite that, they have shortcomings. First, in the evaluation and bibliometrics 
research communities, citations are understood as a metric of attention not of impact or quality. 
That is citations measure the attention to a paper of peers in related fields~\cite{le16} not 
the quality of the work. 
Second, citations might take long to happen. To illustrate, in a recent study that looked at 
first time to citation in a universe of more than a million papers, half of the papers received 
their first citation 20 months or more after their publication˜\cite{n16}. 
Third, citations are not simple to compute and are not always broadly 
available, particularly at the level of individual researchers. 
To illustrate there are now universities requesting their faculty to create a Google Scholar profile with 
the objective of making citations of papers written by an individual researcher readily available. 
However, frequently only a fraction of faculty 
comply\footnote{In a particular instance, a university official complained to one of the authors here 
that just one third of faculty had created Google Scholar profiles.}. 

Despite these well known limitations, citations continue to be broadly used whenever available. 
For instance, while citations are not always available at the level of individual researchers, they are 
widely available at the level of journals and research conferences, that is, at the level of publication 
venues. This is of interest if one takes 
a venue-level metric as a good proxy for the quality of individual 
papers published by that venue, which might be obviously not the case for many papers. 
Despite that, weighting publications by venue-level metrics is a far simpler and more direct 
approach than computing citations for individual researchers. Because of that, venue-level 
metrics are widely used and are of relevance~\cite{laender2008,le16}. 

While venue-level citation metrics are widely available, they do not work well when one is interested 
on a particular subarea of a major area such as Computer Science. For instance, a research funding agency 
might create a program aimed at strengthening research focused in particular subareas such as 
knowledge discovery, data mining, and machine learning. In this case, the question that is difficult to 
answer is which venues better represent research in each of these subareas. In this particular 
case, just considering venue-level citation based metrics does no work well because 
popular journals and conferences in Computer Science frequently span more than one subarea. 
To illustrate consider the subarea of Information Retrieval in Computer Science and the 
venues in which their researchers publish frequently. If just a venue-level citation metric 
is considered, venues such as Jasist (Journal of the American Society in Science and Technology), 
IPM (Information and Processing Management), and WWW (World Wide Web) will show up among the top 10 
venues in Information Retrieval. While these are fine venues, they are not venues on Information Retrieval. 
This is the first problem we address in this work, how to determine automatically a ranking of venues 
in a particular subarea of Computer Science. 

The second problem of our interest here is the fact that citations might take long to appear, as discussed in~\cite{n16}. 
We wonder whether it is possible to notify researchers working on a given subarea of a new venue of interest 
whose reputation is on the rise. We propose a solution and, without loss of generality, explore its application 
to a subset of pre-selected venues. 

\section{P-score: a network-based metric} 

Contrary to citation counting metrics such as the Impact 
Factor~\cite{saha2003,garfield1996,thomson2011,balaban2012positive} and H-index~\cite{benevenuto2016h,bar-ilan2007,egghe2008,bornmann2005does,bornmann2011h}, P-scores are a graph modeling metric that takes into account 
the relations among researchers, papers they published and their publication venues. They are 
based on a framework of reputation flows we introduced previously~\cite{rrsz2015}. Let us review them 
briefly. 

A \emph{reputation graph} in academia is a graph with three node types: (a) {\em reputation sources} representing groups of selected researchers, 
(b) {\em reputation targets} representing venues of interest, and
(c) {\em reputation collaterals} representing entities we want to compare such as research groups and academic 
departments. 
Figure~\ref{fig:overview} provides a generic illustration of our reputation graph and introduces the following 
notation: $S$ is the set of reputation sources, $T$ is the set of reputation targets, and $C$ is the set of reputation collaterals.
We first propagate the reputation of source nodes to target nodes, which allows assigning 
weights to the target nodes. Following, we propagate these weights to the collaterals. 
In here we adopt research groups as source nodes and publication venues as target nodes. 
Further we focus on the weights assigned to the publication venues and do not consider 
potential collaterals of interest (such as individual researchers). 

\begin{figure}[h]
   \centerline{\includegraphics[scale=0.75]{figures/overview-line}}
   %\centerline{\includegraphics[scale=0.6]{figures/models/overview}}
   \caption{Structure of the reputation graph.}
   \label{fig:overview}
\end{figure}

Our reputation graph for academia can be naturally associated with paper authors, modelled as reputation 
sources, and papers, modelled as reputation targets. 
The interaction between reputation sources and reputation targets is inspired by the 
notion of {\em eigenvalue centrality} in complex 
networks~\cite{DBLP:journals/cn/BrinP98,Meyer-pagerank06,Newman-book}. 
In the reputation graph, if we consider only sources and targets, it is easy to identify reputation flows from sources to sources, from sources to targets, from targets to sources, and from targets to targets. These reputation flows can be modeled as a stochastic process. 
In particular, let $P$ be a \emph{right stochastic} %\footnote{A right stochastic matrix is ...} 
matrix of size $(|S|+|T|) \times (|S|+|T|)$ with the following structure: 

\newcommand{\bkt}[1]{ {^{\langle #1 \rangle}} }

\begin{align}\label{eq:newp}
%\[
P =
\left[
\begin{array}{r | r}
(d\bkt{S}) . P\bkt{SS}  & (1-d\bkt{S}) . P\bkt{ST} \\
\hline
(1-d\bkt{T}) . P\bkt{TS}  & (d\bkt{T}) . P\bkt{TT} \\
\end{array}
\right]
%\]
\end{align}
\noindent where each quadrant represents a distinct type of reputation flow, as follows: 

\begin{description}
\item $P\bkt{SS}$: right stochastic matrix of size $|S|\times |S|$ representing the transition probabilities between reputation sources;
\item $P\bkt{ST}$: matrix of size $|S|\times |T|$ representing the transition probabilities from reputation sources to targets;
\item $P\bkt{TS}$: matrix of size $|T|\times |S|$ representing the transition probabilities from reputation targets to sources;
\item $P\bkt{TT}$: right stochastic matrix of size $|T|\times |T|$ representing the transition probabilities between reputation targets.
\end{description}

The parameters $d\bkt{S}$, the fraction of reputation one wants to transfer among the source nodes themselves, 
and $d\bkt{T}$, the fraction of reputation one wants to transfer among the target nodes themselves,
control the relative importance of the reputation sources and targets. 
Assuming that the transition matrix $P$ is ergodic, we can compute the steady state probability of each node and use it as a reputation score. 
More formally, we can write: 
\begin{align}
\label{eq:ggP}
\gamma = \gamma P
\end{align}
\noindent where $\gamma$ is a row matrix with $|S|+|T|$ elements, 
where each row represents the transition probabilities of a node in the set $S\cup T$. 

We should note that, while our network model allows modeling citations in the fourth quadrant, it is possible to compute 
steady state probabilities for the network without consideration to citations. This is accomplished by setting the parameter 
$d\bkt{T} = 0$. 
Thus, it should be clear, in all of our experiments here P-scores 
are computed without taking citations into account. 

%%%%%%%%%%%%% 
\subsection{Reputation Sources}
\label{sec:rep-sources}

The choice of the reputation sources is an important part of the method since its composition has a direct impact in the final rankings. There is no definitive way to make it. This choice depends on what we want to measure. In here we 
use the top CS departments in the US as reputation sources. It is a simple procedure that allows assigning P-scores 
to publication venues, P-scores that 
reflect the publication patterns of top CS departments in the US. 
We then compare how these scores compare with H-indices assigned to the same publication venues. 

One way to determine the top CS departments is to 
adopt the randomization procedure we first described in~\cite{rrsz2015}. A run of that 
procedure works as follows: 
\begin{enumerate}
\item randomly select 10 entities from the set of reputation targets and use them as the set $S$ of reputation sources
\item compute steady state probabilities for all nodes
\item using the steady state probabilities of reputation targets as a score, select the 10 entities with highest scores and use them as a new set $S_{new}$ of reputation sources
\item if $S_{new} \not \equiv S$ then $S \leftarrow S_{new}$ and go back to step 1
\item $S_{auto} \leftarrow S_{new}$ 
\item take $S_{auto}$ as the set of automatically selected reputation sources
\item exit
\end{enumerate}
By applying this randomization procedure 100 times to a set of 125 US graduate programs in Computer Science (CS), 
exactly the same 125 graduate programs considered by NRC in its 2010 evaluation of CS graduate programs 
in the USA~\cite{nrc}, we ended up with a subset of 12 CS programs to be considered as reputation sources. 
These are the CS programs that appeared as least once in the set $S_{auto}$ of automatically selected reputation 
sources and they are as follows: 
%%%
\begin{enumerate}
\item Carnegie Mellon University
\item Georgia Institute of Technology
\item Massachusetts Institute of Technology
\item Stanford University
\item University of California-Berkeley
\item University of California-Los Angeles
\item University of California-San Diego
\item University of Illinois at Urbana-Champaign
\item University of Maryland College Park
\item University of Southern California
\item University of Michigan-Ann Arbor
\item Cornell University
\end{enumerate}
%%%

All the 12 departments above are among the top 5th percentile in the ranking produced by NRC. 
This suggests that our recursive procedure is able to take advantage of patterns in the publication 
streams of the various CS departments to determine the most reputable ones in fully automatic fashion.
We further observe that this was done while setting the parameter $d\bkt{T} = 0$. That is, we did 
not use information on citation counts in the model. 

\section{Correlation with H-indices} 

{\bf TO WRITE: explain that distributions are not Normal, show graphs, propose Kendall-Tau, show macro values}

\section{Assessing conferences in CS}
\label{sec:notifications}

{\bf Discuss the cone strategy, justify the 10 degrees angle, discuss the problem of positioning the cone 
vertex at the origin, introduce the (-100,-100) pivot, discuss items above and below the cone}

\section{Related Work}\label{sec:related-work}

{\bf REWRITE: adjust for theme of this paper}

%%%%%
%%% Ranking academic entities in the broad areas %%%
%%%%%

Citation-based metrics have been widely applied to rank computer and information science journals~\cite{Katerattanakul2003,nerur2005assessing}. Also, different approaches using citation data have been proposed to measure the quality of publication venues in the Databases subarea~\cite{rahm2005citation} and to rank documents retrieved from a digital library~\cite{larsen2006using}. 

Garfield's Impact Factor~\cite{garfield1955} is one of the first metrics proposed to quantify research impact. 
In a nutshell, it indicates the average number of citations per publication of a journal, in the last two years. 
One of the most widespread citation-based metric, the H-index, was proposed by Hirsch~\cite{hirsch2005}. It has been mainly used to rank researchers both in terms of productivity and scientific impact. The key idea behind the H-Index is to detect the quantity of publications of high impact an author has in her research career -- for instance, penalizing authors with a large volume of articles but with a low number of citations for the majority of them. 
Additionally, several works proposed different uses of citation data ~\cite{egghe2006theory, ding-2011, yan2011p, sun2007popularity} and studied their impact, advantages, and disadvantages~\cite{martins2009, leydesdorff2009new}.

The idea of reputation, without the direct use of citation data, was discussed by Nelakuditi et al.~\cite{nelakuditi2011snap}. They proposed a metric called \textit{peers' reputation} for research conferences and journals, which ties the selectivity of the publication venue based upon the reputation of its authors' institutions. The proposed metric was shown to be a better indicator of selectivity of a research venue than acceptance ratio. In addition, the authors observed that, in the subarea of Computer Networks, many conferences have similar or better peers' reputation than journals. This result is similar to the conclusions obtained by Laender et al.~\cite{laender2008}, who show that conference publications are important vehicles for disseminating CS research, while in other areas such as Physical Sciences and Biology the most relevant venues are arguably the scientific journals.

Regarding the assessment of individual researchers' influence and expertise, many approaches have been introduced~\cite{balog2012ftir, cormode2014people, deng2012modeling, gollapalli2011ranking, wu2009detecting}. Particularly, Gonçalves et al.~\cite{goncalves2014} quantified the impact of various features on a scholar popularity throughout her career. 
They concluded that, even though most of the considered features are strongly correlated with popularity, only two features are needed to explain almost all the variation in popularity across different researchers: the number of publications and the average quality of the scholar’s publication venues. 
In addition, the prediction of scientific success of a researcher is also valuable for several goals
~\cite{masoumeh16}. As a result, previous works attempted to predict if a researcher will become a principal investigator~\cite{vandijk14}, her future H-index~\cite{dong15, penner13} and the potential number of citations to her publications~\cite{mazloumian12, castillo2007}.

Although citation-based metrics are useful, they are not enough to do a complete evaluation of research. 
In particular, Piwowar~\cite{piwowar2013altmetrics} showed that metrics as the H-Index are slow, as the first citation of a scientific article can take years. He concludes that the development of alternative metrics to complement citation analysis is not only desirable, but a necessity. 

The reputation model we use in this work was proposed 
in 
~\cite{ribas2015random}. This model, called \textit{reputation flows}, exploits the transference of reputation among entities in order to identify the most reputable ones. Particularly, the reputation flows consist in a random walk model where the reputation of a target set of entities is inferred using suitable sources of reputation. To evaluate this model, they instantiated the reputation flows in an academic setting, proposing a novel metric for academic reputation, the \textit{P-score}~\cite{ribas2015bigscholar}. 

By and large, the aforementioned works or variations of them are commonly used in assessments of academic output and also by modern search engines for scientific digital libraries, e.g. Google Scholar\footnote{\url{https://scholar.google.com.br}}, Microsoft Academic Search\footnote{\url{http://academic.microsoft.com}}, AMiner\footnote{\url{http://aminer.org}}, %maintained by Tang et al.~\cite{Tang:08KDD},
and CiteSeerX\footnote{\url{http://citeseerx.ist.psu.edu}}.  
However, %all of those performance 
none of the referred metrics take into account the different publication patterns in the subareas. Studies suggesting those differences and the negative impact of uniform evaluation metrics have been discussed in the field of Economics~\cite{kapeller10, lee10} and in Computer Science~\cite{hoonlor13, laendersigs15, lima2013jcdl}.

Wainer et al.~\cite{wainer13} presented the first attempt to quantify the differences in publication and citation practices between the subareas of Computer Science. 
Their key findings were: i) there are significant differences in productivity across some CS subareas, both in journals (e.g. Bioinformatics has a significantly higher productivity than Artificial Intelligence) and in conferences (e.g. Image Processing and Computer Vision has a significantly higher productivity than Operational Research and Optimization); ii) the mean number of citations per paper varies depending on subarea (e.g. Management Information Systems has significantly higher citation rates per paper than Computer Architecture); and iii) there are significant differences in emphasis on publishing in journals or in conferences (e.g. Bioinformatics are clearly journal oriented while Artificial Intelligence are conference oriented). However, they do not focus on modeling a new productivity metric for academic domain taking into account those differences between the subareas.

%In summary, 
To the best of our knowledge, this is the first work that tackles the problem of both identifying the most important venues of a subarea in Computer Science and rank research groups based on this information, in a semi-automatic fashion.

\section{Conclusions}\label{sec:conclusions}

ZZZ Rewrite entirely.

\begin{comment}
\begin{acks}

ZZZ Thank scholarships by CAPES and CNPq.

\end{acks}
\end{comment}
%% To fill space (Ueda)
\section*{ACKNOWLEDGEMENTS}
Omitted for blind review.

%\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{abbrv}
\bibliography{pscore}

\end{document}

